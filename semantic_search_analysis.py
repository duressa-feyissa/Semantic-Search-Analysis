# -*- coding: utf-8 -*-
"""Semantic Search Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IdZ6y3BbbjBL1IMgOxtZOaaMOvIwSYTo

# **Semantic Search Analysis**
"""

import pandas as pd
import re
import nltk
import spacy
import pandas as pd
import joblib
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('names')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

nlp = spacy.load('en_core_web_sm')

df = pd.read_csv('text.csv')
df.rename(columns={'Unnamed: 0': 'id'}, inplace=True)
df.head()

def tokenize(text):
    return nltk.word_tokenize(text)

def normalize(text):
    text = text.lower()
    text = re.sub(r'\b\w{1,2}\b', '', text)
    text = re.sub(r'\d+', '', text)
    return text

def remove_stopwords(words):
    stop_words = set(stopwords.words('english'))
    return [word for word in words if word not in stop_words]

def Stemming(words):
    stemmer = PorterStemmer()
    return [stemmer.stem(word) for word in words]

def Lemmatization(words):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in words]

def pos_tagging(words):
    return nltk.pos_tag(words)

def preprocess_text(text):
    text = normalize(text)
    words = tokenize(text)
    words = remove_stopwords(words)
    words = Stemming(words)
    words = Lemmatization(words)
    return ' '.join(words)

df['processed_text'] = df['text'].apply(preprocess_text)

def named_entity_recognition(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

df.head()

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['processed_text'])

def query_system(query, vectorizer, tfidf_matrix, df, top_k=5):

    preprocessed_query = preprocess_text(query)

    query_vector = vectorizer.transform([preprocessed_query])

    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

    top_indices = similarities.argsort()[-top_k:][::-1]

    results = [(df.iloc[i]['id'], df.iloc[i]['text'], similarities[i]) for i in top_indices]

    return results

def extract_named_entities(text):
    entities = named_entity_recognition(text)
    for entity, label in entities:
        print(f"Entity: {entity}, Label: {label}")

def user_query(query):
  results = query_system(query, vectorizer, tfidf_matrix, df)

  for idx, result in enumerate(results):
      print(f"Result {idx+1}:")
      print(f"Document ID: {result[0]}")
      print(f"Text: {result[1]}\n")
      print(f"Similarity Score: {result[2]:.4f}\n")
      print("Named Entities:")
      extract_named_entities(result[1])
      print("\n")

  return [ result[1] for result in results ]

query = "I feel lost and helpless"
user_query(query)

joblib.dump(vectorizer, 'vectorizer.joblib')
df.to_pickle('processed_data.pkl')